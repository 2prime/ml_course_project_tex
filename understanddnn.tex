\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{comment}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{natbib}
\usepackage{graphicx}

\usepackage{geometry}
\geometry{left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{rem}{Remark}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\bracket}[1]{\left(#1\right)}
\newcommand{\sgn}[1]{\text{sgn}\left(#1\right)}
\newcommand{\dis}{\displaystyle}
\newcommand{\tr}{\text{tr}}
\newcommand{\diag}{\text{diag}}
\newcommand{\Diag}{\text{Diag}}
\newcommand{\cond}{\text{cond}}
\newcommand{\conv}{\text{conv}}
\newcommand{\rank}{\text{rank}}
\newcommand{\dist}{\text{dist}}
\newcommand{\dom}{\text{dom}}
\newcommand{\epi}{\text{epi}}
\newcommand{\Prox}{\text{Prox}}
\newcommand{\Env}{\text{Env}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\iprod}[2]{\left\langle #1,#2 \right\rangle}

\title{Understanding The Mystery Of Deep Learning Loss Surface}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
	Siyu Chen, Yiping Lu, Tianle Cai, Kewen Wu\\
Peking University
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Deep neural networks have become the state-of-the-art
models in numerous machine learning
tasks. However, there is less theory to give the guarantee for both the optimization process and the generalization properties. In this technical report, we focus mainly on the optimization perspective, \emph{i.e.}, the guarantee for the computational complexity of the optimization algorithm for deep neural networks. First we consider an important factor \emph{depth} in deep learning. Due to the non-convexity of the optimization problem, deeper doesn't lead to better predictor even only evaluating on training data, although deeper networks have stronger approximation ability. But our work shows that deep ResNet can reaches lower training loss than a shallow one. Next we will show that the right label, \emph{i.e.}, the regularity of the objective function to approximate, matters when we consider the properties of the loss surface.

\end{abstract}

\section{The deeper the lower training loss}

Residual Networks \cite{he2016deep,he2016identity}, which introduced a identity mapping based shortcut, is a class of deep neural networks and provide state-of-the-art performance both in image classification\cite{he2016deep,he2016identity}, image reconstruction\cite{he2016deep}, deep reinforcement learning\cite{silver2017mastering} and etc. ResNets allow the training of each layer only needs to focus on fitting just the residual of the previous layerâ€™s output and the target output. \cite{2018arXiv180406739S} proves that for all deep residual, networks' local minimum is smaller than a linear predictor's global minimum. It shows that the trained network is no worse than what we can obtain if we remove the
residual layers and train a shallower network instead. In this section, we extend the result in \cite{2018arXiv180406739S} which shows that for a two layer neural network, there is also a similar statement holds, \emph{i.e.}, a learned two layer residual network is better than a one layer network. To describe the result formally, we first introduce some notation and definitions.

\begin{defn}[loss function]
    $\ell(\cdot;\cdot)$ takes two scalars as input. We assume $\ell(p;y)=(p-y)^2$.
\end{defn}

\begin{defn}[loss of two-layer Resnet]
	We assume $f_1$ is a function without any parameter. Let $f_{2,\theta}$ denote a twice differentiable function $f_2$ with parameter $\theta$, then the loss of a two-layer resnet can be written as:
	\[L_2(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2,\theta;\mathbf{x},y):=\ell\left(\mathbf{w}^T(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})+\mathbf{V}_2f_{2,\theta}(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})));y\right)\]
	When fixing $\theta$, we define
	\[L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y):=L_2(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2,\theta;\mathbf{x},y)\]
	We also define the expected loss over a distribution of $\mathbf{x}$ and $y$ as
	\[F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2):=\mathbb{E}_{\mathbf{x},y}L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)\]
\end{defn}

\begin{defn}[loss of one-layer Resnet]
	\begin{align*}
	L_1(\mathbf{w},\mathbf{V}_1;\mathbf{x},y)&:=L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{0};\mathbf{x},y)=\ell\left(\mathbf{w}^T(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x}));y\right)\\
	F_1(\mathbf{w},\mathbf{V}_1)&:=\mathbb{E}_{\mathbf{x},y}L_1(\mathbf{w},\mathbf{V}_1;\mathbf{x},y)=F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{0})
	\end{align*}
\end{defn}

\begin{flushleft}
	Let $\norm{\mathbf{x}}$ denote the Euclidean norm of vector $\mathbf{x}$, $\norm{\mathbf{X}}_2,\norm{\mathbf{X}}_F$ denote the spectral norm and the Frobenius norm of matrix $\mathbf{X}$ respectively, and $\mathbf{x}\circ\mathbf{y}$ denote the entry-wise product of $\mathbf{x}$ and $\mathbf{y}$.
\end{flushleft}

\begin{lem}
	Fix some $\mathbf{w},\mathbf{V}_1,\mathbf{V}_2,\theta$,
    where $\mathbf{w}\neq\mathbf{0}$.
    Assume for any $\mathbf{x}$, $\norm{f_1(\mathbf{x})}_\infty<\frac c 2$.
    For any $\mathbf{w}^*$,$\mathbf{V}_1^*$, define the matrix
    \[\mathbf{G}=
    \left(\mathbf{w}-\mathbf{w}^*;
    \mathbf{0};
    \frac{\mathbf{w}\left(\mathbf{w}^{*}\right)^T}{\norm{\mathbf{w}}^2}\mathbf{V}_2\right)\]
	then for any $\mathbf{x},y$,
    \[\iprod{\text{vec}(\mathbf{G})}
    {\text{vec}\left(\nabla F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)\right)}
    +\delta
    \geq F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)
    -F_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y),\]
    where
    \[\delta=c\norm{\left(\mathbf{w}^*\right)^T\left(\mathbf{V}_1-\mathbf{V}_1^*\right)}_1
    \sqrt{F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)}.\]
\end{lem}

\begin{proof}
	We define $d\ell(\mathbf{x},y)=\dis\frac{\partial}{\partial p}\ell(p;y)
    \bigg|_{p=\mathbf{w}^T(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})+\mathbf{V}_2f_{2,\theta}(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})))}$ in order to simplify notation.

	It can be easily verified that
	\begin{align*}
	\frac{\partial}{\partial\mathbf{w}}L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)&=d\ell(\mathbf{x},y)(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})+\mathbf{V}_2f_{2,\theta}(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})))\\
	\frac{\partial}{\partial\mathbf{V}_2}L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)&=d\ell(\mathbf{x},y)\mathbf{w}f_{2,\theta}(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x}))^T
	\end{align*}
	Using the definition of $\mathbf{G}$, we can do the following calculation:
	\begin{align*}
	&\iprod{\text{vec}(\mathbf{G})}{\text{vec}\left(\nabla L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)\right)}\\
	=&d\ell(\mathbf{x},y)\left(\mathbf{w}^T(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})+\mathbf{V}_2f_{2,\theta}(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})))-\left(\mathbf{w}^*\right)^T(\mathbf{x}+\mathbf{V}_1^*f_1(\mathbf{x}))\right)\\
        &\quad +d\ell(\mathbf{x},y)\left(\left(\mathbf{w}^*\right)^T\left(\mathbf{V}_1^*-\mathbf{V}_1\right)f_1(\mathbf{x})\right)\\
	\geq &L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)-L_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y)
        +d\ell(\mathbf{x},y)\left(\left(\mathbf{w}^*\right)^T\left(\mathbf{V}_1^*-\mathbf{V}_1\right)f_1(\mathbf{x})\right)\\
	\geq &L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)-L_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y)
        -\abs{d\ell(\mathbf{x},y)\left(\left(\mathbf{w}^*\right)^T\left(\mathbf{V}_1^*-\mathbf{V}_1\right)f_1(\mathbf{x})\right)}\\
	\geq &L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)-L_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y)
        -\frac c 2\abs{d\ell(\mathbf{x},y)}\norm{\left(\mathbf{w}^*\right)^T\left(\mathbf{V}_1^*-\mathbf{V}_1\right)}_1\\
	\end{align*}
	The first inequality uses the convexity of $\ell(\cdot;\cdot)$ in its first argument.

    Since $\ell(p;y)=(p-y)^2$, we have $d\ell(p;y)=2(p-y)$. And take expectation over $\mathbf{x}$ and $y$ on both sides yields
    \begin{align*}
    &\iprod{\text{vec}(\mathbf{G})}
        {\text{vec}\left(\nabla F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)\right)}\\
    \geq & F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)
    -F_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y)
    -c\norm{\left(\mathbf{w}^*\right)^T\left(\mathbf{V}_1^*-\mathbf{V}_1\right)}_1\gamma,
    \end{align*}
    where
    \[
        \gamma=\mathbb{E}_{\mathbf{x},y}
        \abs{\mathbf{w}^T\big(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})
        +\mathbf{V}_2f_{2,\theta}\left(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})\right)\big)-y}.
    \]
    Using Jensen inequality, we have
    \begin{align*}
        \gamma & \leq \sqrt{\mathbb{E}_{\mathbf{x},y}
        \Big(\mathbf{w}^T\big(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})
        +\mathbf{V}_2f_{2,\theta}\left(\mathbf{x}+\mathbf{V}_1f_1(\mathbf{x})\right)\big)-y\Big)^2}\\
        & = \sqrt{F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)}.
    \end{align*}
    Thus the claim.
\end{proof}

\begin{prop}
	Suppose $\eta\in(0,1)$. At any point $(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2,\theta)$ such that $\mathbf{w}\neq\mathbf{0}$, $\norm{\mathbf{V}_2}_2\leq 1-\eta$ and $\norm{\nabla f_{2,\theta}}_\infty\leq 1$, and for any $\mathbf{w}^*$,$\mathbf{V}_1^*$,
	\[\mathbb{E}_{\mathbf{x},y}\norm{\nabla L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)}\geq\frac{F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2)-F_1(\mathbf{w}^*,\mathbf{V}_1^*)}{\sqrt{2\norm{\mathbf{w}}^2+\norm{\mathbf{w}^*}^2\left(2+\dis\frac{1}{\norm{\mathbf{w}}^2}\left(\norm{\mathbf{V}_2}_2^2+\frac{\norm{\mathbf{V}_1-\mathbf{V}_1^*}_2^2}{\eta^2}\right)\right)}}\]
	The term $\norm{\mathbf{V}_2}_2^2$ can be replaced by $(1-\eta)^2$.
\end{prop}

\begin{proof}
	Using Cauchy-Schwarz inequality and Lemma 1, we have
	\begin{align}
	\norm{\mathbf{G}(\mathbf{x})}_F\norm{\nabla L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)}\geq L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)-L_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y)
	\label{ineq1}
	\end{align}
	When $\norm{\mathbf{V}_2}_2\leq 1-\eta$,
	\[\norm{\mathbf{u}(\mathbf{x})}\geq \norm{\mathbf{w}}\left(1-\norm{\mathbf{V}_2}_2\right)\geq\eta\norm{\mathbf{w}}\]
	Thus for any $\mathbf{x}$,
	\begin{align*}
	\norm{\mathbf{G}(\mathbf{x})}_F^2&\leq\norm{\mathbf{w}-\mathbf{w}^*}^2+\frac{\norm{\mathbf{w^*}}^2\norm{\mathbf{V}_2}_2^2}{\norm{\mathbf{w}}^2}+\frac{\norm{\mathbf{w^*}}^2\norm{\mathbf{V}_1-\mathbf{V}_1^*}_2^2}{\norm{\mathbf{u}(\mathbf{x})}^2}\\
	&\leq2\norm{\mathbf{w}}^2+2\norm{\mathbf{w}^*}^2+\frac{\norm{\mathbf{w}^*}^2\norm{\mathbf{V}_2}_2^2}{\norm{\mathbf{w}}^2}+\frac{\norm{\mathbf{w^*}}^2\norm{\mathbf{V}_1-\mathbf{V}_1^*}_2^2}{\eta^2\norm{\mathbf{w}}^2}
	\end{align*}
	Then we combine the inequality above and Eq.(\ref{ineq1}):
	\begin{align*}
	&\sqrt{2\norm{\mathbf{w}}^2+\norm{\mathbf{w}^*}^2\left(2+\frac{1}{\norm{\mathbf{w}}^2}\left(\norm{\mathbf{V}_2}_2^2+\frac{\norm{\mathbf{V}_1-\mathbf{V}_1^*}_2^2}{\eta^2}\right)\right)}\norm{\nabla L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)}\\
	&\geq L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)-L_1(\mathbf{w}^*,\mathbf{V}_1^*;\mathbf{x},y)
	\end{align*}
	Taking expectation over $\mathbf{x}$ and $y$ on both sides yields the result.
\end{proof}

\begin{rem}
	We want to prove a theorem similar to Theorem 1 in \cite{2018arXiv180406739S}, but end up with this proposition. It is weaker since
	\[\mathbb{E}_{\mathbf{x},y}\norm{\nabla L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)}\geq\norm{\mathbb{E}_{\mathbf{x},y}\left[\nabla L_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2;\mathbf{x},y)\right]}=\norm{\nabla F_{2,\theta}(\mathbf{w},\mathbf{V}_1,\mathbf{V}_2)}\]
	but we do not know how to improve it.
\end{rem}

\section{Better target function leads to better loss surface}

\cite{cicek2018saas,cicek2018input} demonstrates the idea that "the regularity of the target function leads to the smoothness of the loss surface" and they designed semi-supervised learning algorithms based on this observation, \emph{i.e.}, learning speed as a supervisor. We reproduce the experiment that bad quality of given label will lead to slower training speed and the results is shown in Figure \ref{saas}.

\begin{figure}[htp]
	\centering
	\includegraphics[width=2.5in]{cifar.jpeg}
	\includegraphics[width=2.5in]{mnist.jpg}
	\caption{Comparison of training the neural network to fit the given label and fit a random labeled dataset using \textbf{gradient descent}. Left hand side is the figure for DenseNet101 training on CIFAR10 dataset and right hand side the one for LeNet5 training on MNIST. It seems that there exists a saddle point around $\log 10$. \textbf{All the loss we evaluate after the last layer is the cross-entropy loss.}}
	\label{saas}
\end{figure}

Figure \ref{saas} shows that the loss of dataset labeled with ground truth drops quickly to zero even when we use the gradient descent algorithm without any noise, which implies that the loss surface in this case is pretty good. However, if we label the data with random choices, the learning curve changes a lot. The loss first quickly drops to a place around $\log 10$ and stops here for a long time. Once the loss leaves $\log 10$, the loss drops faster. This phenomenon suggests that the learning process was going through a saddle point. Repeating the experiment several times, we find that $\log10$ seems a magic number, that all training process starting from a random initialization hits a saddle point near $\log 10$. As the loss function for the last layer output we denote in the experiment is the cross-entropy loss, $\log 10$ is the place where we give a random guess.


\subsection{A Counter Example}

Inspired by the magic number $\log 10$, in this section we give a counter example that there exists a weight, which is a first order condition but not a global minimum for random guess label.

In this section, we use the $\ell_2$ loss as the loss function between the target label and the network output. That is to say our objective function is designed as $\argmin_w\mathbb{E}_{(x,y)\sim Dataset}\left\|f_w(x)-y\right\|_2^2$. %If the dataset is generated via two independent random variable $x,y$ and $P(y=1)=P(y=-1)=1/2$, then $f_w(x)=0$ for all $x$ is a point that satisfies the first order condition.

\begin{table}[H]
\begin{minipage}[t]{0.4\linewidth}
\flushright
\begin{tabular}{|c|c|c|}
\hline $x^\mathrm{T}$&$y$&$y'$\\
\hline $(0,0,0)$&0&0\\
\hline $(0,0,1)$&1&0\\
\hline $(0,1,0)$&1&0\\
\hline $(0,1,1)$&1&0\\
\hline $(1,0,0)$&0&1\\
\hline $(1,0,1)$&0&1\\
\hline $(1,1,0)$&1&1\\
\hline $(1,1,1)$&0&1\\
\hline
\end{tabular}
\end{minipage}
\begin{minipage}[t]{0.5\linewidth}
\centering
\begin{tabular}{|c|c|}
\hline $W_1$&$W_2$\\
\hline (-2,1,0)&1\\
\hline
\end{tabular}
\end{minipage}
\\
\caption{Counter example: The network structure is simple two layer feedforward neural network with activation function "polished ReLU" which is based on ReLU but smooth at 0's neighbor and has derivative 0 at 0. Specifically, the network works as: $f_w(x)=W_2\sigma(W_1 x)$. Then for data $(x,y)$ whose label is randomly drawn, the weights $W_1, W_2$ reach a point that satisfies the first order condition, yet it is not a global minimum (when $W_1'=(-2,1,1), W_2'=1$, the loss is less than the example) . Meanwhile $W_1, W_2$ is not a critical point for data $(x,y')$ whose label obviously depends on $x$ (indeed coincide with the first coordinate of $x$) which means the optimization will not stuck at the $W_1, W_2$ above for dependent data.}
\end{table}

%\textbf{it seem that this construction is independent of the network structure. It only needs there exist a weight to let the network output is all 0.}

%\textbf{write it formally.}

%\textbf{I trust you can construct an example. First a weight outputs 0, ex. all the neuron is dead and let several pairs of data is close and half of them is 0 and half of them is 1. next over-paramize(para>data) to construct a smaller value.}


\subsection{Experiments}





\subsection{Data Doesn't Matter}

The success of convolution operator in neural networks is contributes to "the feature of an image depends on the local relationship in an image" and "convolution operator is invariant to translation" in common sense. However, our experiment shows that these factors may not contribute to a nice loss surface. In order to break the translation invariance and local relationship, we randomly permute the pixels in an image and doesn't change the given label. The learning curve is demonstrated in Figure \ref{randomperb}.

\begin{figure}[htp]
	\centering
	\includegraphics[width=2.5in]{gdrp.jpg}
	\includegraphics[width=2.5in]{adamrp.jpg}
	\caption{Learning curves for the random permuted dataset and original one. Left hand side is the learning curve of gradient descent algorithm and the right hand side the ADAM algorithm.}
	\label{randomperb}
\end{figure}


\section{Loss surface for one data}

In the next section, we discuss the sufficiency of the cooperation of the data to give a guarantee for a good loss landscape. In this section, we consider the landscape when there is only one data.


\subsection{Vanilla feedforward neural networks}
Consider the loss function of a vanilla feed forward neural network,

$$f(W_1,\cdots,W_n)=\left\|W_n\sigma (W_{n-1}\sigma(W_{n-2}\cdots\sigma(W_1x)))-y\right\|_2^2.$$

Here $x$ is the input data and $y$ is the associated label. $W_i$ is the weight of the $i$-th label and $\sigma$ is the activation function.


$$
\frac{\partial f}{\partial W_i}=\hat\sigma((W_n\sigma (W_{n-1}\sigma(W_{n-2}\cdots\sigma(W_1x)))-y)W_n^t)\cdots
$$

\textbf{check it!}

Can we give some assumption that leads to for some $i$

$$
\frac{\partial f}{\partial W_i} \ge c|(W_n\sigma (W_{n-1}\sigma(W_{n-2}\cdots\sigma(W_1x)))-y|
$$

\subsection{Deep Residual Networks}

Base on the observation that deep residual network $x_{k+1}=x_{k}+f(x_k)$ can be considered as the forward Euler scheme for an ODE, recent works begin to utilize the ODE to model the deep networks. The learning process can be understand as solving a control problem. There are a lot of work focusing on finding sufficient conditions for the optimal control. In this section, we briefly introduce some of the results and hope to bring some insight for understanding the loss surface of the neural networks. All the theorem in this section can be found in \cite{bressan2007introduction}.

First we would like to introduce the first order principle, \emph{i.e.} $\nabla f=0$, for the control problem.
\begin{prop}
	\textbf{(Pontryagin Maximum Principle)} Consider the optimization problem $$\min_{u\in\mathbf{U}} \phi(x(T,u))$$ for the control problem described by
	$$\dot x = f(x,t,u),x(0)=\hat{x},u(t)\in\mathbf{U},t\in [0,T]$$

	Let $u^*$ be a bounded admissible control, whose corresponding trajectory $x^*(\cdot)$ is optimal for the optimization problem. Then there exists a nontrivial, absolutely continuous vector function $p(\cdot)$ which satisfies
	$$\dot{p}(t)=-p(t)\cdot D_xf(t,x^*(t),u^*(t))$$
	$$p(t)\cdot f(t,x^*(t),u^*(t))=\max_{u\in\mathbf{U}}\left\{p(t)\cdot f(t,x^*(t),\omega)\right\}$$
	at almost every time $t\in[0,T]$, together with the terminal conditions
	$$p(T)=\lambda_0\nabla\phi_i(x^*(T))$$
	for some constants $\lambda_0$.
\end{prop}

Previous works have try to find sufficient conditions for optimal control and there are results claims that if the terminal cost function $\phi$ is convex and Pontryagin Maximum Principle holds, the control is an optimal control.


\begin{prop}
	In addition if the functional $u\rightarrow \phi_0(x(T,u))$ from $\mathbf{U}$ into $\mathbb{R}$ is convex. Then for any trajectory satisfies the tryagin's equation is optimal.
\end{prop}

This means for a given weight of the last layer, the loss surface seems good. The first order condition(PMP) can imply an optimal control, which is also been proved in \cite{bartlett2018representing} in discrete sense.

There is another another type of conditions is utilizing the definition of value function $$V(s,y)=\inf_{u\in\mathbf{U_{s,y}}}\psi(T,x(T,u,s,y))$$ where $\mathbf{U_{s,y}}$ denotes all the trajectory satisfies $x(s)=y$. The value function satisfies the Hamilton-Jacobi equation $$\partial_sV(s,y)+\inf_{\omega\in\mathbf{U}}\left\{\nabla_yV(s,y)\cdot f(s,y,\omega)\right\}=0$$
\subsection{Experiments}

\section{Further Directions}

\subsection{Fast Algorithms Escaping Saddle Points}

We also test different algorithms (Momentum SGD, ADAM and AdaDelta) to escape the saddle point around $\log 10$ and the result is demonstrated in Figure \ref{optsadd}. The result shows that both ADAM and AdaDelta can escape from the saddle point and ADAM escapes faster. Can we analyze the escape rate for the AdaDelta? (For ADAM, it has a momentum scheme on gradient which is hard to analyze in the non-convex setting. There are recent papers analyzing AdaGrad in non-convex setting.) There are several factors influencing the speed of accelerating the escaping process:

\begin{itemize}
	\item The adaptive learning rate is large in ADAM and AdaDelta. When stuck in a saddle point, $H_{t+1}=(1-m)H_t+m\hat{g_t}^2$ will become smaller exponentially which means the stepsize increases exponentially.
	\item The adaptive learning rate also estimates the column-wise curvature of the loss surface. If the curvature is estimated correctly, the learning steps will go through the direction associated largest eigenvalue of the Hessian which is the fastest direction to escape the saddle point.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=2.5in]{opt.jpg}
	\includegraphics[width=2.5in]{saddle.jpg}
	\caption{Comparison between different optimization algorithm to escape the saddle point around $\log 10$. We test Momentum SGD, ADAM and AdaDelta. ADAM and AdaDelta is able to escape from the saddle point and ADAM performs better. The right hand side figure is a zoom-in version of the first one.}
	\label{optsadd}
\end{figure}

\subsection{Fast (Decoupled) Optimization Algorithm Based On The Control Perspective}

There are works showing that we can learn a neural network even if we fixed the last layer's weights. The learning problem turns to a standard control problem. This is natural to apply a \textbf{reinforcement learning} algorithm here. In the previous section, we have introduced that the \textbf{value function} provides a sufficient condition for the control problem. It's natural to use a \textbf{Q-learning} algorithm here to decouple the gradient computation during the back propagation, for the Bellman equation only depends on the previous state the following state.


Another view (It may be another approach) of this kind of decouple learning algorithm is that we can adopt the domain decomposition technique to parallelize the computation of the backward ODE in PMP. Rewrite the adjoint equation in PMP here

$$
\dot p(t) = -p(t)\cdot D_xf(t,x(t),u(t)),p(T)=\nabla \psi(x(T))
$$

Equally, the equation can be written as

$$
\dot p_1(t) = -p_1(t)\cdot D_xf(t,x(t),u(t))|_{t\in[T/2,T]},p_1(T)=\nabla \psi(x(T))$$
$$
\dot p_2(t) = -p_2(t)\cdot D_xf(t,x(t),u(t))|_{t\in[0,T/2]},p_2(T/2)=p_1(T/2)
$$

If we adopt a Gaussian-Seidel like algorithm, we decouple the gradient computation between $[0,T/2]$ and $[T/2,T]$.

\subsection{Proper assumption for label and data assumption}

In our paper, we claim that all the data should cooperate in order to get a good gradient, but how can we define the cooperation of gradients in mathematics.

\subsection{Combine Generalization}

\subsection{Loss surface near the converge point}
\cite{DBLP:journals/corr/abs-1712-10132} shows the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities has a multilinear structure. Specifically, we can find the loss is a piecewise multilinear form. Base on this observation, we are now interested in the behavior of the weights that jumps between the pieces during optimization. We do some experience and count the times of "jump" (the activation function ReLU's value gotten from different linear part) during the optimization and found that the times increase as the optimization going on. At the same time the jump times seems converge, which may indicate that the optimization lead to a area at the edge of many pieces of the loss surface. This also makes us rethink what we get from optimization.

\begin{figure}[H]
	\centering
	\includegraphics[width=2.5in]{PieceJump.jpg}
	\caption{Times of activation state change of ReLU(from 0 to positive) per epoch. We use Lenet and test on Cifar10 database.}
	\label{piecejump}
\end{figure}

\bibliographystyle{plain}
\bibliography{provably}

\end{document}
